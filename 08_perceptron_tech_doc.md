# 感知机技术文档

## 1. 感知机的概念

感知机（Perceptron）是由美国学者Frank Rosenblatt在1957年提出的，是神经网络和深度学习的起源算法。感知机是一种简单的二分类线性模型，也是最简单的人工神经网络。

感知机是一种接收多个输入信号，输出一个信号的模型。

**基本结构**：

感知机接收多个输入信号 $x_1, x_2, ..., x_n$，每个输入信号都有对应的权重 $w_1, w_2, ..., w_n$，输出信号 $y$。

**数学表达式**：

$$y = \begin{cases} 0 & (w_1x_1 + w_2x_2 + ... + w_nx_n \leq \theta) \\ 1 & (w_1x_1 + w_2x_2 + ... + w_nx_n > \theta) \end{cases}$$

其中：
- $x_i$：输入信号
- $w_i$：权重（weight），表示各个信号的重要程度
- $\theta$：阈值（threshold），决定神经元是否激活
- $y$：输出信号

**引入偏置的形式**：

将阈值 $\theta$ 替换为偏置 $b = -\theta$，感知机可以表示为：

$$y = \begin{cases} 0 & (w_1x_1 + w_2x_2 + ... + w_nx_n + b \leq 0) \\ 1 & (w_1x_1 + w_2x_2 + ... + w_nx_n + b > 0) \end{cases}$$

或者用向量形式：

$$y = \begin{cases} 0 & (\mathbf{w}^T\mathbf{x} + b \leq 0) \\ 1 & (\mathbf{w}^T\mathbf{x} + b > 0) \end{cases}$$

**权重和偏置的作用**：
- **权重**：控制输入信号的重要程度，权重越大，对应输入信号的影响越大
- **偏置**：调整神经元被激活的容易程度，偏置越大，越容易输出1

## 2. 简单逻辑电路

感知机可以用来实现简单的逻辑电路，如与门、与非门、或门等。

### 2.1 与门

与门（AND gate）是一种基本逻辑门，只有当所有输入都为1时，输出才为1。

**真值表**：

| $x_1$ | $x_2$ | $y$ |
|-------|-------|-----|
| 0 | 0 | 0 |
| 0 | 1 | 0 |
| 1 | 0 | 0 |
| 1 | 1 | 1 |

**感知机实现**：

```python
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1 * w1 + x2 * w2
    if tmp <= theta:
        return 0
    else:
        return 1

# 测试
print(AND(0, 0))  # 输出: 0
print(AND(1, 0))  # 输出: 0
print(AND(0, 1))  # 输出: 0
print(AND(1, 1))  # 输出: 1
```

**参数选择说明**：
- 当 $(w_1, w_2, \theta) = (0.5, 0.5, 0.7)$ 时，只有 $x_1 = x_2 = 1$ 时，$0.5 + 0.5 = 1.0 > 0.7$，输出1
- 其他组合如 $(0.5, 0.5, 0.8)$ 或 $(1.0, 1.0, 1.0)$ 也可以实现与门

### 2.2 与非门

与非门（NAND gate）是与门的输出取反，即只有当所有输入都为1时，输出才为0。

**真值表**：

| $x_1$ | $x_2$ | $y$ |
|-------|-------|-----|
| 0 | 0 | 1 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

**感知机实现**：

```python
def NAND(x1, x2):
    w1, w2, theta = -0.5, -0.5, -0.7
    tmp = x1 * w1 + x2 * w2
    if tmp <= theta:
        return 0
    else:
        return 1

# 测试
print(NAND(0, 0))  # 输出: 1
print(NAND(1, 0))  # 输出: 1
print(NAND(0, 1))  # 输出: 1
print(NAND(1, 1))  # 输出: 0
```

**参数说明**：与非门的参数只需要将与门的参数全部取反即可。

### 2.3 或门

或门（OR gate）是一种基本逻辑门，只要有一个输入为1，输出就为1。

**真值表**：

| $x_1$ | $x_2$ | $y$ |
|-------|-------|-----|
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 1 |

**感知机实现**：

```python
def OR(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.3
    tmp = x1 * w1 + x2 * w2
    if tmp <= theta:
        return 0
    else:
        return 1

# 测试
print(OR(0, 0))  # 输出: 0
print(OR(1, 0))  # 输出: 1
print(OR(0, 1))  # 输出: 1
print(OR(1, 1))  # 输出: 1
```

## 3. 感知机的实现

### 3.1 简单实现

使用阈值方式实现感知机：

```python
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1 * w1 + x2 * w2
    if tmp <= theta:
        return 0
    else:
        return 1
```

### 3.2 导入权重和偏置

将阈值转换为偏置，使用NumPy实现更规范的感知机：

```python
import numpy as np

# 与门
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7  # 偏置 b = -theta
    res = w @ x + b  # 使用矩阵乘法
    if res <= 0:
        return 0
    else:
        return 1

# 与非门
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    res = w @ x + b
    if res <= 0:
        return 0
    else:
        return 1

# 或门
def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.2
    res = w @ x + b
    if res <= 0:
        return 0
    else:
        return 1
```

**代码说明**：
- 使用NumPy数组表示输入和权重
- 使用 `w @ x` 矩阵乘法计算加权和，等价于 `np.dot(w, x)` 或 `np.sum(w * x)`
- 偏置 $b = -\theta$，将判断条件从 $\sum w_ix_i > \theta$ 转换为 $\sum w_ix_i + b > 0$
- 这种形式与神经网络中的表示方式一致

## 4. 感知机的局限

单层感知机只能表示**线性可分**的问题，无法解决**线性不可分**的问题。

**异或门（XOR gate）问题**：

异或门的真值表：

| $x_1$ | $x_2$ | $y$ |
|-------|-------|-----|
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

**为什么单层感知机无法实现异或门？**

单层感知机的决策边界是一条直线（在二维空间中），表示为：

$$w_1x_1 + w_2x_2 + b = 0$$

对于异或门，需要将 $(0,0)$、$(1,1)$ 分为一类（输出0），$(0,1)$、$(1,0)$ 分为另一类（输出1）。

在二维平面上，这四个点无法用一条直线分开，因此单层感知机无法实现异或门。

**几何解释**：
- 与门、或门、与非门的输出点可以用一条直线分开（线性可分）
- 异或门的输出点无法用一条直线分开（线性不可分）

## 5. 多层感知机

为了解决单层感知机的局限性，可以使用**多层感知机（Multi-Layer Perceptron, MLP）**。

**异或门的多层实现**：

异或门可以通过组合与门、或门、与非门来实现：

$$XOR(x_1, x_2) = AND(NAND(x_1, x_2), OR(x_1, x_2))$$

**验证**：

| $x_1$ | $x_2$ | NAND | OR | AND(NAND, OR) |
|-------|-------|------|----|----|
| 0 | 0 | 1 | 0 | 0 |
| 0 | 1 | 1 | 1 | 1 |
| 1 | 0 | 1 | 1 | 1 |
| 1 | 1 | 0 | 1 | 0 |

**代码实现**：

```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y

# 测试
print(XOR(0, 0))  # 输出: 0
print(XOR(1, 0))  # 输出: 1
print(XOR(0, 1))  # 输出: 1
print(XOR(1, 1))  # 输出: 0
```

**多层感知机的结构**：

异或门的多层感知机包含：
- **第0层（输入层）**：$x_1$, $x_2$
- **第1层（隐藏层）**：NAND和OR的输出 $s_1$, $s_2$
- **第2层（输出层）**：AND的输出 $y$

**多层感知机的意义**：
- 通过叠加层，感知机可以表示更复杂的函数
- 单层感知机只能表示线性函数，多层感知机可以表示非线性函数
- 理论上，多层感知机可以逼近任意连续函数（万能近似定理）

**从感知机到神经网络**：
- 感知机是神经网络的基础
- 神经网络通过增加层数和使用可微的激活函数，实现了更强大的表达能力
- 深度学习就是使用多层神经网络进行学习
