# 其他监督学习算法技术文档（了解）

## 1. 朴素贝叶斯法

朴素贝叶斯（Naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类方法。它是一种简单但非常有效的分类算法，广泛应用于文本分类、垃圾邮件过滤等领域。

### 1.1 朴素贝叶斯法简介

朴素贝叶斯法是一种基于概率的分类算法，其核心思想是根据已知类别的先验概率和条件概率，计算样本属于各个类别的后验概率，选择后验概率最大的类别作为预测结果。

**贝叶斯定理**：

$$P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$$

其中：
- $P(Y|X)$：后验概率，给定特征$X$时类别为$Y$的概率
- $P(X|Y)$：似然概率，类别为$Y$时特征为$X$的概率
- $P(Y)$：先验概率，类别$Y$的概率
- $P(X)$：证据因子，特征$X$的概率

**"朴素"的含义**：

朴素贝叶斯假设各个特征之间相互独立，即：

$$P(X|Y) = P(x_1, x_2, ..., x_n|Y) = \prod_{i=1}^{n}P(x_i|Y)$$

这个假设虽然在实际中很难成立，但朴素贝叶斯在很多场景下仍然表现良好。

**分类决策**：

$$\hat{y} = \arg\max_{y} P(Y=y) \prod_{i=1}^{n}P(x_i|Y=y)$$

### 1.2 极大似然估计

极大似然估计（Maximum Likelihood Estimation, MLE）是一种参数估计方法，用于估计朴素贝叶斯模型中的概率参数。

**先验概率估计**：

$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}$$

其中$I$为指示函数，$N$为样本总数。

**条件概率估计**：

对于离散特征：
$$P(X_j=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^{N}I(x_{ij}=a_{jl}, y_i=c_k)}{\sum_{i=1}^{N}I(y_i=c_k)}$$

### 1.3 贝叶斯估计

贝叶斯估计是对极大似然估计的改进，通过引入拉普拉斯平滑（Laplace Smoothing）来解决零概率问题。

**平滑后的先验概率**：

$$P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k) + \lambda}{N + K\lambda}$$

其中$K$为类别数，$\lambda \geq 0$为平滑参数（通常取$\lambda=1$）。

**平滑后的条件概率**：

$$P(X_j=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^{N}I(x_{ij}=a_{jl}, y_i=c_k) + \lambda}{\sum_{i=1}^{N}I(y_i=c_k) + S_j\lambda}$$

其中$S_j$为第$j$个特征的取值个数。

### 1.4 学习与分类过程

**学习过程**：

1. 计算先验概率$P(Y=c_k)$
2. 计算条件概率$P(X_j=a_{jl}|Y=c_k)$
3. 存储所有概率参数

**分类过程**：

1. 对于新样本$x = (x_1, x_2, ..., x_n)$
2. 计算各类别的后验概率：$P(Y=c_k)\prod_{j=1}^{n}P(x_j|Y=c_k)$
3. 选择后验概率最大的类别作为预测结果

## 2. 决策树

决策树（Decision Tree）是一种基于树结构进行决策的分类和回归方法，具有很好的可解释性。

### 2.1 决策树简介

决策树是一种树形结构的分类模型，通过对特征进行测试，从根节点开始，根据测试结果选择相应的分支，直到到达叶节点，叶节点代表最终的分类结果。

**基本组成**：
- **根节点**：包含全部样本的初始节点
- **内部节点**：对应特征测试
- **分支**：对应特征测试的输出
- **叶节点**：对应分类结果

**优点**：
- 模型可解释性强
- 可以处理数值型和类别型特征
- 不需要特征缩放
- 能够自动进行特征选择

**缺点**：
- 容易过拟合
- 对数据变化敏感
- 可能生成复杂的树结构

### 2.2 决策树工作过程

**构建过程**：

1. 从根节点开始，计算所有特征的信息增益（或其他指标）
2. 选择信息增益最大的特征进行分裂
3. 对每个分支递归执行上述步骤
4. 直到满足停止条件（如纯度达标、样本数过少等）

**预测过程**：

1. 从根节点开始
2. 根据当前节点的特征测试条件，选择对应的分支
3. 重复步骤2直到到达叶节点
4. 返回叶节点的类别标签

### 2.3 特征选择与决策树生成

**信息熵**：

$$H(D) = -\sum_{k=1}^{K}\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$$

**信息增益（ID3算法）**：

$$g(D, A) = H(D) - H(D|A)$$

其中$H(D|A)$为特征$A$给定条件下$D$的条件熵。

**信息增益比（C4.5算法）**：

$$g_R(D, A) = \frac{g(D, A)}{H_A(D)}$$

**基尼指数（CART算法）**：

$$Gini(D) = 1 - \sum_{k=1}^{K}\left(\frac{|C_k|}{|D|}\right)^2$$

### 2.4 决策树的剪枝

剪枝是防止决策树过拟合的重要手段。

**预剪枝**：
- 在构建决策树的过程中，提前停止树的生长
- 设置最大深度、最小样本数等阈值

**后剪枝**：
- 先生成完整的决策树，再自底向上剪枝
- 通过验证集评估剪枝后的性能

## 3. 支持向量机

支持向量机（Support Vector Machine, SVM）是一种强大的分类算法，其核心思想是找到能够最大化类别间隔的最优超平面。

### 3.1 支持向量机简介

SVM的基本思想是在特征空间中找到一个超平面，使得两类样本被正确分开，且间隔最大。

**超平面**：

$$\mathbf{w}^T\mathbf{x} + b = 0$$

**分类决策函数**：

$$f(\mathbf{x}) = sign(\mathbf{w}^T\mathbf{x} + b)$$

**支持向量**：
- 距离超平面最近的样本点
- 决定了超平面的位置
- 是SVM的关键元素

### 3.2 线性可分支持向量机-硬间隔

当数据线性可分时，使用硬间隔最大化方法。

**间隔**：

$$\gamma = \frac{2}{\|\mathbf{w}\|}$$

**优化目标**：

$$\min_{\mathbf{w}, b} \frac{1}{2}\|\mathbf{w}\|^2$$

$$s.t. \quad y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1, \quad i = 1, 2, ..., N$$

**求解方法**：通过拉格朗日对偶问题求解。

### 3.3 线性支持向量机-软间隔

当数据近似线性可分时，允许一些样本不满足约束条件。

**引入松弛变量**：

$$\min_{\mathbf{w}, b, \xi} \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^{N}\xi_i$$

$$s.t. \quad y_i(\mathbf{w}^T\mathbf{x}_i + b) \geq 1 - \xi_i$$
$$\xi_i \geq 0, \quad i = 1, 2, ..., N$$

其中$C > 0$为惩罚参数，控制间隔最大化和误分类惩罚之间的平衡。

### 3.4 非线性支持向量机-核函数

当数据线性不可分时，使用核函数将数据映射到高维空间。

**核函数**：

$$K(\mathbf{x}_i, \mathbf{x}_j) = \phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$$

**常用核函数**：

| 核函数 | 表达式 |
|--------|--------|
| 线性核 | $K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T\mathbf{x}_j$ |
| 多项式核 | $K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i^T\mathbf{x}_j + c)^d$ |
| 高斯核(RBF) | $K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\sigma^2})$ |
| Sigmoid核 | $K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\beta\mathbf{x}_i^T\mathbf{x}_j + \theta)$ |

## 4. 集成学习

集成学习（Ensemble Learning）通过组合多个学习器来获得比单一学习器更好的性能。

### 4.1 AdaBoost

AdaBoost（Adaptive Boosting）是一种自适应增强算法，通过迭代训练多个弱分类器，并根据分类效果调整样本权重。

**算法流程**：

1. 初始化样本权重：$w_1^{(1)} = w_2^{(1)} = ... = w_N^{(1)} = \frac{1}{N}$

2. 对于$m = 1, 2, ..., M$：
   - 使用权重分布$D_m$训练弱分类器$G_m(\mathbf{x})$
   - 计算分类误差率：$e_m = \sum_{i=1}^{N}w_i^{(m)}I(G_m(\mathbf{x}_i) \neq y_i)$
   - 计算分类器权重：$\alpha_m = \frac{1}{2}\ln\frac{1-e_m}{e_m}$
   - 更新样本权重：$w_i^{(m+1)} = \frac{w_i^{(m)}\exp(-\alpha_m y_i G_m(\mathbf{x}_i))}{Z_m}$

3. 最终分类器：$G(\mathbf{x}) = sign\left(\sum_{m=1}^{M}\alpha_m G_m(\mathbf{x})\right)$

**特点**：
- 自适应调整样本权重，关注难分类样本
- 弱分类器的权重与其分类准确率相关
- 不容易过拟合

### 4.2 随机森林

随机森林（Random Forest）是一种基于决策树的集成学习方法，通过构建多棵决策树并综合其预测结果。

**算法流程**：

1. 从原始数据集中有放回地抽取$N$个样本（Bootstrap采样）
2. 在每个节点分裂时，随机选择$k$个特征（$k < n$，$n$为总特征数）
3. 根据选中的特征，选择最优分裂点构建决策树
4. 重复步骤1-3，构建$T$棵决策树
5. 预测时，对所有树的预测结果进行投票（分类）或取平均（回归）

**随机性来源**：
- **样本随机**：Bootstrap采样
- **特征随机**：节点分裂时随机选择特征子集

**优点**：
- 能够处理高维数据
- 不容易过拟合
- 可以评估特征重要性
- 训练速度快，可以并行化

**重要参数**：
- `n_estimators`：决策树的数量
- `max_features`：每次分裂时随机选择的特征数
- `max_depth`：决策树的最大深度
- `min_samples_split`：节点分裂所需的最小样本数
